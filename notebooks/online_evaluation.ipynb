{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Online Evaluation - Recommendation System Telemetry\n",
    "\n",
    "This notebook performs automated online evaluation of the recommendation system by analyzing:\n",
    "- Service performance metrics (latency, error rates, success rates)\n",
    "- Model performance metrics (coverage, diversity, personalization, conversion)\n",
    "- Data quality and telemetry\n",
    "\n",
    "**Usage**: Run all cells to generate a complete telemetry report.\n",
    "\n",
    "**Data Source**: S3 events from `s3://shrek-events-dev/events/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "env-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials setup\n",
    "import os\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIATX3PICZ6L3ULH47Y\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "spark-init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark session initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 56931)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socketserver.py\", line 766, in __init__\n",
      "    self.handle()\n",
      "    ~~~~~~~~~~~^^\n",
      "  File \"/Users/lenghanz/Downloads/group-project-f25-shrekommender-system/.venv/lib/python3.13/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "    ~~~~^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lenghanz/Downloads/group-project-f25-shrekommender-system/.venv/lib/python3.13/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ~~~~^^\n",
      "  File \"/Users/lenghanz/Downloads/group-project-f25-shrekommender-system/.venv/lib/python3.13/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/lenghanz/Downloads/group-project-f25-shrekommender-system/.venv/lib/python3.13/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"online_evaluation\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.0,com.amazonaws:aws-java-sdk-bundle:1.12.698\"\n",
    "    )\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .config(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Hadoop S3 access configuration\n",
    "hconf = spark._jsc.hadoopConfiguration()\n",
    "hconf.set(\"fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.EnvironmentVariableCredentialsProvider\")\n",
    "hconf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "print(\"✓ Spark session initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis date: 2025-10-26\n",
      "Base path: s3a://shrek-events-dev/events/\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "from datetime import datetime\n",
    "\n",
    "# Date to analyze (modify as needed)\n",
    "ANALYSIS_DATE = \"2025-10-26\"\n",
    "\n",
    "# S3 paths\n",
    "BASE_PATH = \"s3a://shrek-events-dev/events/\"\n",
    "PATHS = [\n",
    "    f\"s3a://shrek-events-dev/events/date={ANALYSIS_DATE}/event_type=rate\",\n",
    "    f\"s3a://shrek-events-dev/events/date={ANALYSIS_DATE}/event_type=recommendation\",\n",
    "    f\"s3a://shrek-events-dev/events/date={ANALYSIS_DATE}/event_type=watch\",\n",
    "]\n",
    "\n",
    "print(f\"Analysis date: {ANALYSIS_DATE}\")\n",
    "print(f\"Base path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADING SUMMARY\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 30,490,595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watch events: 30,079,387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate events: 159,400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(109 + 2) / 111]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation events: 251,808\n",
      "✓ Data loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load all event data\n",
    "from pyspark.sql.functions import col, when, sum as spark_sum, count as spark_count, avg as spark_avg\n",
    "\n",
    "df = spark.read.option(\"basePath\", BASE_PATH).parquet(*PATHS)\n",
    "\n",
    "# Separate by event type\n",
    "watch_df = df.filter(df.event_type == \"watch\")\n",
    "rate_df = df.filter(df.event_type == \"rate\")\n",
    "rec_df = df.filter(df.event_type == \"recommendation\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total records: {df.count():,}\")\n",
    "print(f\"Watch events: {watch_df.count():,}\")\n",
    "print(f\"Rate events: {rate_df.count():,}\")\n",
    "print(f\"Recommendation events: {rec_df.count():,}\")\n",
    "print(\"✓ Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "service-perf-header",
   "metadata": {},
   "source": [
    "## Service Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "service-overview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SERVICE PERFORMANCE OVERVIEW\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=====================================================>(102 + 1) / 103]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Requests: 225,616\n",
      "Successful Requests: 224,366\n",
      "Failed Requests: 1,250\n",
      "Success Rate: 99.45%\n",
      "Error Rate: 0.55%\n",
      "Unique Users Served: 166,039\n",
      "Active Servers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Overall service metrics\n",
    "print(\"=\"*80)\n",
    "print(\"SERVICE PERFORMANCE OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_requests = rec_df.count()\n",
    "successful_requests = rec_df.filter(col(\"status_code\") == 200).count()\n",
    "failed_requests = total_requests - successful_requests\n",
    "unique_users = rec_df.select(\"user_id\").distinct().count()\n",
    "unique_servers = rec_df.select(\"server\").distinct().count()\n",
    "\n",
    "print(f\"\\nTotal Requests: {total_requests:,}\")\n",
    "print(f\"Successful Requests: {successful_requests:,}\")\n",
    "print(f\"Failed Requests: {failed_requests:,}\")\n",
    "print(f\"Success Rate: {(successful_requests / total_requests * 100):.2f}%\")\n",
    "print(f\"Error Rate: {(failed_requests / total_requests * 100):.2f}%\")\n",
    "print(f\"Unique Users Served: {unique_users:,}\")\n",
    "print(f\"Active Servers: {unique_servers:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "status-codes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STATUS CODE DISTRIBUTION\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " status_code  count  percentage\n",
      "           0   1250        0.55\n",
      "         200 224366       99.45\n"
     ]
    }
   ],
   "source": [
    "# Status code breakdown\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATUS CODE DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "status_breakdown = rec_df.groupBy(\"status_code\").count().orderBy(\"status_code\")\n",
    "status_breakdown_pd = status_breakdown.toPandas()\n",
    "status_breakdown_pd['percentage'] = (status_breakdown_pd['count'] / total_requests * 100).round(2)\n",
    "print(status_breakdown_pd.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "error-by-hour",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ERROR RATE BY HOUR\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=====================================================>(102 + 1) / 103]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hour  total_requests  error_count  error_rate\n",
      "    2            3855            2        0.05\n",
      "    3           12347            4        0.03\n",
      "    4           11485            9        0.08\n",
      "    5           11041            4        0.04\n",
      "    6           11126            7        0.06\n",
      "    7           11751            4        0.03\n",
      "    8           12842            5        0.04\n",
      "    9           14287            5        0.03\n",
      "   10           15855           20        0.13\n",
      "   11           18346            6        0.03\n",
      "   12           19997            3        0.02\n",
      "   13           22128            5        0.02\n",
      "   14           24242         1174        4.84\n",
      "   15           25408            2        0.01\n",
      "   16           10906            0        0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Error rate over time\n",
    "from pyspark.sql.functions import hour\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ERROR RATE BY HOUR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "error_by_hour = rec_df.withColumn(\"hour\", hour(\"timestamp\")) \\\n",
    "    .groupBy(\"hour\") \\\n",
    "    .agg(\n",
    "        spark_count(\"*\").alias(\"total_requests\"),\n",
    "        spark_sum(when(col(\"status_code\") != 200, 1).otherwise(0)).alias(\"error_count\")\n",
    "    ) \\\n",
    "    .orderBy(\"hour\") \\\n",
    "    .toPandas()\n",
    "\n",
    "error_by_hour['error_rate'] = (error_by_hour['error_count'] / error_by_hour['total_requests'] * 100).round(2)\n",
    "print(error_by_hour.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-perf-header",
   "metadata": {},
   "source": [
    "## Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coverage-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL COVERAGE ANALYSIS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Users requesting recommendations: 166,039\n",
      "Cold start users (no history): 1,607 (0.97%)\n",
      "Warm users (with history): 164,432 (99.03%)\n",
      "Highly engaged users (watch + rate): 102,587 (61.78%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:====================================================> (101 + 2) / 103]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total movies in catalog: 33,865\n",
      "Movies being recommended: 1,025\n",
      "Catalog coverage: 3.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Model coverage analysis\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COVERAGE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# User coverage\n",
    "rec_user_set = set(rec_df.select(\"user_id\").rdd.flatMap(lambda x: x).collect())\n",
    "watch_user_set = set(watch_df.select(\"user_id\").rdd.flatMap(lambda x: x).collect())\n",
    "rate_user_set = set(rate_df.select(\"user_id\").rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "cold_start_users = rec_user_set - watch_user_set - rate_user_set\n",
    "warm_users = rec_user_set.intersection(watch_user_set.union(rate_user_set))\n",
    "highly_engaged = rec_user_set.intersection(watch_user_set).intersection(rate_user_set)\n",
    "\n",
    "print(f\"\\nUsers requesting recommendations: {len(rec_user_set):,}\")\n",
    "print(f\"Cold start users (no history): {len(cold_start_users):,} ({len(cold_start_users)/len(rec_user_set)*100:.2f}%)\")\n",
    "print(f\"Warm users (with history): {len(warm_users):,} ({len(warm_users)/len(rec_user_set)*100:.2f}%)\")\n",
    "print(f\"Highly engaged users (watch + rate): {len(highly_engaged):,} ({len(highly_engaged)/len(rec_user_set)*100:.2f}%)\")\n",
    "\n",
    "# Item coverage\n",
    "total_movies = df.select(\"movie_id\").distinct().count()\n",
    "recommended_movies = rec_df.filter(col(\"recommendations\").isNotNull()).select(\"recommendations\").distinct().count()\n",
    "\n",
    "print(f\"\\nTotal movies in catalog: {total_movies:,}\")\n",
    "print(f\"Movies being recommended: {recommended_movies:,}\")\n",
    "print(f\"Catalog coverage: {recommended_movies/total_movies*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "personalization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERSONALIZATION ANALYSIS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total successful requests: 224,366\n",
      "Distinct recommendation sets: 1,022\n",
      "Personalization Score: 0.46%\n",
      "\n",
      "⚠️  WARNING: Very low personalization detected\n",
      "   Only 1022 unique recommendations for 224,366 users\n",
      "\n",
      "Most common recommendations:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:=====================================================>(102 + 1) / 103]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------+\n",
      "|recommendations                                      |count |\n",
      "+-----------------------------------------------------+------+\n",
      "|the+shawshank+redemption+1994                        |212132|\n",
      "|the+lord+of+the+rings+the+two+towers+2002            |201   |\n",
      "|the+lion+king+1994                                   |187   |\n",
      "|interstellar+2014                                    |155   |\n",
      "|kikis+delivery+service+1989                          |142   |\n",
      "|my+neighbor+totoro+1988                              |142   |\n",
      "|constantine+2005                                     |141   |\n",
      "|the+lord+of+the+rings+the+fellowship+of+the+ring+2001|140   |\n",
      "|the+dark+knight+2008                                 |117   |\n",
      "|inception+2010                                       |114   |\n",
      "+-----------------------------------------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Personalization metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERSONALIZATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "successful_recs = rec_df.filter(col(\"status_code\") == 200)\n",
    "distinct_recommendation_sets = successful_recs.select(\"recommendations\").distinct().count()\n",
    "total_recommendation_requests = successful_recs.count()\n",
    "\n",
    "print(f\"\\nTotal successful requests: {total_recommendation_requests:,}\")\n",
    "print(f\"Distinct recommendation sets: {distinct_recommendation_sets:,}\")\n",
    "print(f\"Personalization Score: {distinct_recommendation_sets/total_recommendation_requests*100:.2f}%\")\n",
    "\n",
    "if distinct_recommendation_sets == 1:\n",
    "    print(\"\\n⚠️  CRITICAL: ALL users are receiving IDENTICAL recommendations!\")\n",
    "    print(\"   This indicates ZERO personalization - model may be using fallback strategy\")\n",
    "elif distinct_recommendation_sets / total_recommendation_requests < 0.1:\n",
    "    print(\"\\n⚠️  WARNING: Very low personalization detected\")\n",
    "    print(f\"   Only {distinct_recommendation_sets} unique recommendations for {total_recommendation_requests:,} users\")\n",
    "else:\n",
    "    print(\"\\n✓ Recommendations show reasonable personalization\")\n",
    "\n",
    "# Show top recommendations\n",
    "print(\"\\nMost common recommendations:\")\n",
    "successful_recs.groupBy(\"recommendations\").count().orderBy(\"count\", ascending=False).limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "diversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RECOMMENDATION DIVERSITY\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:=====================================================>(102 + 1) / 103]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique movies recommended: 1,022\n",
      "Total recommendation instances: 224,366\n",
      "Average times per movie recommended: 219.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Recommendation diversity\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION DIVERSITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "movie_rec_freq = successful_recs.groupBy(\"recommendations\").count().orderBy(\"count\", ascending=False)\n",
    "unique_movies_recommended = movie_rec_freq.count()\n",
    "\n",
    "print(f\"\\nUnique movies recommended: {unique_movies_recommended:,}\")\n",
    "print(f\"Total recommendation instances: {total_recommendation_requests:,}\")\n",
    "\n",
    "if unique_movies_recommended > 0:\n",
    "    print(f\"Average times per movie recommended: {total_recommendation_requests / unique_movies_recommended:.2f}\")\n",
    "\n",
    "if unique_movies_recommended == 1:\n",
    "    print(\"\\n⚠️  CRITICAL: Only ONE movie being recommended to all users!\")\n",
    "    print(\"   Model has likely fallen back to most-popular fallback strategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion analysis - CORRECTED to match specific recommended movies\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION CONVERSION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "users_with_recs = set(rec_df.filter(col(\"status_code\") == 200).select(\"user_id\").distinct().rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "print(\"Analyzing conversion for SPECIFIC recommended movies (not any movie)...\")\n",
    "\n",
    "# CORRECT METHOD: Join on both user_id AND movie_id (recommendations)\n",
    "# This ensures we're tracking if users watched/rated the SPECIFIC movie that was recommended\n",
    "\n",
    "# Prepare recommendation data with movie_id\n",
    "rec_with_movie = rec_df.filter(col(\"status_code\") == 200).select(\n",
    "    \"user_id\", \n",
    "    col(\"recommendations\").alias(\"movie_id\"),  # recommendations field is the movie_id\n",
    "    col(\"timestamp\").alias(\"rec_time\")\n",
    ")\n",
    "\n",
    "# Prepare watch/rate data with movie_id\n",
    "watch_with_movie = watch_df.select(\"user_id\", \"movie_id\", col(\"timestamp\").alias(\"watch_time\"))\n",
    "rate_with_movie = rate_df.select(\"user_id\", \"movie_id\", col(\"timestamp\").alias(\"rate_time\"))\n",
    "\n",
    "# Join on BOTH user_id AND movie_id to ensure same movie\n",
    "# Filter to only count actions AFTER the recommendation\n",
    "rec_to_watch = rec_with_movie.join(watch_with_movie, [\"user_id\", \"movie_id\"], \"inner\") \\\n",
    "    .filter(col(\"watch_time\") > col(\"rec_time\"))\n",
    "\n",
    "rec_to_rate = rec_with_movie.join(rate_with_movie, [\"user_id\", \"movie_id\"], \"inner\") \\\n",
    "    .filter(col(\"rate_time\") > col(\"rec_time\"))\n",
    "\n",
    "# Count unique users who converted (watched/rated the SPECIFIC recommended movie)\n",
    "converted_watch_users = rec_to_watch.select(\"user_id\").distinct().count()\n",
    "converted_rate_users = rec_to_rate.select(\"user_id\").distinct().count()\n",
    "\n",
    "# Also count total conversion instances (a user might watch same recommended movie multiple times)\n",
    "total_watch_conversions = rec_to_watch.count()\n",
    "total_rate_conversions = rec_to_rate.count()\n",
    "\n",
    "print(f\"\\nUsers receiving recommendations: {len(users_with_recs):,}\")\n",
    "print(f\"\\n### SPECIFIC Movie Conversion (Recommended Movie Only) ###\")\n",
    "print(f\"Users who watched the RECOMMENDED movie: {converted_watch_users:,}\")\n",
    "print(f\"Users who rated the RECOMMENDED movie: {converted_rate_users:,}\")\n",
    "print(f\"Total watch conversion instances: {total_watch_conversions:,}\")\n",
    "print(f\"Total rate conversion instances: {total_rate_conversions:,}\")\n",
    "\n",
    "if len(users_with_recs) > 0:\n",
    "    print(f\"\\n### Conversion Rates ###\")\n",
    "    print(f\"Watch conversion rate (specific movie): {converted_watch_users/len(users_with_recs)*100:.2f}%\")\n",
    "    print(f\"Rate conversion rate (specific movie): {converted_rate_users/len(users_with_recs)*100:.2f}%\")\n",
    "    print(f\"\\nNote: This measures users who watched/rated the EXACT movie that was recommended,\")\n",
    "    print(f\"      not just any movie after receiving a recommendation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "quality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RECOMMENDATION QUALITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Recommended movies with quality metrics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+------------------+------------+\n",
      "|recommendations                                    |avg_rating        |rating_count|\n",
      "+---------------------------------------------------+------------------+------------+\n",
      "|leon+the+professional+1994                         |4.010869565217392 |184         |\n",
      "|gone+girl+2014                                     |4.027397260273973 |146         |\n",
      "|die+hard+1988                                      |3.9591836734693877|98          |\n",
      "|the+spongebob+movie+sponge+out+of+water+2015       |3.259259259259259 |27          |\n",
      "|master+and+commander+the+far+side+of+the+world+2003|3.671875          |64          |\n",
      "|the+black+cauldron+1985                            |3.3846153846153846|13          |\n",
      "|thomas+pynchon+a+journey+into+the+mind+of+p.+2003  |3.5               |2           |\n",
      "|when+we+were+kings+1996                            |4.571428571428571 |7           |\n",
      "|grease+1978                                        |3.8536585365853657|41          |\n",
      "|the+lion+king+1994                                 |3.885496183206107 |131         |\n",
      "|ponyo+2008                                         |3.8358208955223883|134         |\n",
      "|the+princess+diaries+2001                          |2.8947368421052633|19          |\n",
      "|the+little+mermaid+1989                            |4.015873015873016 |63          |\n",
      "|interstellar+2014                                  |4.1152647975077885|8346        |\n",
      "|spirited+away+2001                                 |4.22096858184343  |5761        |\n",
      "|the+hunchback+of+notre+dame+1996                   |3.4583333333333335|24          |\n",
      "|princess+mononoke+1997                             |4.051724137931035 |174         |\n",
      "|akira+1988                                         |3.7534246575342465|73          |\n",
      "|army+1944                                          |3.0               |4           |\n",
      "|planet+terror+2007                                 |4.0               |2           |\n",
      "+---------------------------------------------------+------------------+------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 113:====================================================>(102 + 1) / 103]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "High quality movies in catalog (avg rating >= 4.0, min 5 ratings): 721 / 17,146 (4.21%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Recommendation quality\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION QUALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate average ratings for movies\n",
    "movie_avg_ratings = rate_df.groupBy(\"movie_id\") \\\n",
    "    .agg(\n",
    "        spark_avg(\"rating\").alias(\"avg_rating\"),\n",
    "        spark_count(\"rating\").alias(\"rating_count\")\n",
    "    )\n",
    "\n",
    "# Join with recommended movies\n",
    "rec_with_recs = rec_df.filter(col(\"recommendations\").isNotNull())\n",
    "recs_with_quality = rec_with_recs.select(\"recommendations\").distinct() \\\n",
    "    .join(\n",
    "        movie_avg_ratings,\n",
    "        col(\"recommendations\") == col(\"movie_id\"),\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "print(\"\\nRecommended movies with quality metrics:\")\n",
    "recs_with_quality.select(\"recommendations\", \"avg_rating\", \"rating_count\").show(20, truncate=False)\n",
    "\n",
    "# High quality movies count\n",
    "high_quality_movies = movie_avg_ratings.filter(\"avg_rating >= 4.0 AND rating_count >= 5\")\n",
    "total_rated_movies = movie_avg_ratings.count()\n",
    "high_quality_count = high_quality_movies.count()\n",
    "\n",
    "print(f\"\\nHigh quality movies in catalog (avg rating >= 4.0, min 5 ratings): {high_quality_count:,} / {total_rated_movies:,} ({high_quality_count/total_rated_movies*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary\n",
    "print(\"=\"*80)\n",
    "print(\"EXECUTIVE SUMMARY - ONLINE EVALUATION REPORT\")\n",
    "print(f\"Analysis Date: {ANALYSIS_DATE}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n### SERVICE HEALTH ###\")\n",
    "print(f\"Total Requests: {total_requests:,}\")\n",
    "print(f\"Success Rate: {(successful_requests / total_requests * 100):.2f}%\")\n",
    "print(f\"Error Rate: {(failed_requests / total_requests * 100):.2f}%\")\n",
    "\n",
    "# Service health status\n",
    "if (successful_requests / total_requests) >= 0.99:\n",
    "    print(\"Status: ✓ EXCELLENT\")\n",
    "elif (successful_requests / total_requests) >= 0.95:\n",
    "    print(\"Status: ✓ GOOD\")\n",
    "elif (successful_requests / total_requests) >= 0.90:\n",
    "    print(\"Status: ⚠️  DEGRADED\")\n",
    "else:\n",
    "    print(\"Status: ❌ CRITICAL\")\n",
    "\n",
    "print(\"\\n### MODEL PERFORMANCE ###\")\n",
    "print(f\"Personalization Score: {distinct_recommendation_sets/total_recommendation_requests*100:.2f}%\")\n",
    "print(f\"Unique Movies Recommended: {unique_movies_recommended:,}\")\n",
    "print(f\"Catalog Coverage: {recommended_movies/total_movies*100:.2f}%\")\n",
    "print(f\"Cold Start Users: {len(cold_start_users)/len(rec_user_set)*100:.2f}%\")\n",
    "print(f\"Watch Conversion Rate (specific movie): {converted_watch_users/len(users_with_recs)*100:.2f}%\")\n",
    "print(f\"Rate Conversion Rate (specific movie): {converted_rate_users/len(users_with_recs)*100:.2f}%\")\n",
    "\n",
    "# Model health status\n",
    "if distinct_recommendation_sets == 1:\n",
    "    print(\"Status: ❌ CRITICAL - No personalization\")\n",
    "elif distinct_recommendation_sets / total_recommendation_requests < 0.1:\n",
    "    print(\"Status: ⚠️  WARNING - Low personalization\")\n",
    "elif distinct_recommendation_sets / total_recommendation_requests < 0.5:\n",
    "    print(\"Status: ⚠️  DEGRADED - Limited personalization\")\n",
    "else:\n",
    "    print(\"Status: ✓ GOOD\")\n",
    "\n",
    "print(\"\\n### KEY INSIGHTS ###\")\n",
    "\n",
    "insights = []\n",
    "\n",
    "if distinct_recommendation_sets == 1:\n",
    "    insights.append(\"⚠️  All users receiving identical recommendations - investigate model loading\")\n",
    "\n",
    "if len(cold_start_users) / len(rec_user_set) > 0.5:\n",
    "    insights.append(f\"⚠️  High cold-start rate ({len(cold_start_users)/len(rec_user_set)*100:.1f}%) - many users have no history\")\n",
    "\n",
    "if (failed_requests / total_requests) > 0.05:\n",
    "    insights.append(f\"⚠️  High error rate ({failed_requests/total_requests*100:.1f}%) - investigate service issues\")\n",
    "\n",
    "# Updated to use specific movie conversion rate\n",
    "if converted_watch_users / len(users_with_recs) > 0.2:\n",
    "    insights.append(f\"✓ Good conversion rate ({converted_watch_users/len(users_with_recs)*100:.1f}%) - users engaging with recommended movies\")\n",
    "elif converted_watch_users / len(users_with_recs) > 0.1:\n",
    "    insights.append(f\"⚠️  Moderate conversion rate ({converted_watch_users/len(users_with_recs)*100:.1f}%) - room for improvement\")\n",
    "else:\n",
    "    insights.append(f\"⚠️  Low conversion rate ({converted_watch_users/len(users_with_recs)*100:.1f}%) - recommendations may not be relevant\")\n",
    "\n",
    "if recommended_movies / total_movies < 0.01:\n",
    "    insights.append(f\"⚠️  Very low catalog coverage ({recommended_movies/total_movies*100:.2f}%) - most movies not recommended\")\n",
    "\n",
    "if distinct_recommendation_sets / total_recommendation_requests < 0.01:\n",
    "    insights.append(f\"⚠️  Very low personalization ({distinct_recommendation_sets/total_recommendation_requests*100:.2f}%) - most users get same recommendations\")\n",
    "\n",
    "if not insights:\n",
    "    insights.append(\"✓ No major issues detected\")\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Report generated at:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session (uncomment if needed)\n",
    "# spark.stop()\n",
    "# print(\"✓ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shrekommender-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
